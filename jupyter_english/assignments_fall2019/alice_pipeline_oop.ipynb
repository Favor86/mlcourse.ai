{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file, target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = np.arange(1, predicted_labels.shape[0] + 1), columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path('../../data/alice/')\n",
    "\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv(PATH_TO_DATA / 'train_sessions.csv',\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv(PATH_TO_DATA / 'test_sessions.csv',\n",
    "                      index_col='session_id', parse_dates=times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2013-01-12 08:05:57'),\n",
       " Timestamp('2014-04-30 23:39:53'),\n",
       " Timestamp('2014-05-01 17:14:03'),\n",
       " Timestamp('2014-12-05 23:26:53'))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1'].min(), train_df['time1'].max(), test_df['time1'].min(), test_df['time1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 21), (82797, 20))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82797 entries, 1 to 82797\n",
      "Data columns (total 20 columns):\n",
      "site1     82797 non-null int64\n",
      "time1     82797 non-null datetime64[ns]\n",
      "site2     81308 non-null float64\n",
      "time2     81308 non-null datetime64[ns]\n",
      "site3     80075 non-null float64\n",
      "time3     80075 non-null datetime64[ns]\n",
      "site4     79182 non-null float64\n",
      "time4     79182 non-null datetime64[ns]\n",
      "site5     78341 non-null float64\n",
      "time5     78341 non-null datetime64[ns]\n",
      "site6     77566 non-null float64\n",
      "time6     77566 non-null datetime64[ns]\n",
      "site7     76840 non-null float64\n",
      "time7     76840 non-null datetime64[ns]\n",
      "site8     76151 non-null float64\n",
      "time8     76151 non-null datetime64[ns]\n",
      "site9     75484 non-null float64\n",
      "time9     75484 non-null datetime64[ns]\n",
      "site10    74806 non-null float64\n",
      "time10    74806 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](10), float64(9), int64(1)\n",
      "memory usage: 13.3 MB\n"
     ]
    }
   ],
   "source": [
    "#train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fill NaN with zero values.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        sites = ['site%s' % i for i in range(1, 11)]\n",
    "        return X[sites].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListPreparator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Prepare a CountVectorizer friendly 2D-list from data.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.values.tolist()\n",
    "        # Convert dataframe rows to strings\n",
    "        return [\" \".join([str(site) for site in row]) for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add new attributes to training and test set.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        # intraday features\n",
    "        hour = X['time1'].apply(lambda ts: ts.hour)\n",
    "        morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "        day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "        evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "        \n",
    "        # season features\n",
    "        month = X['time1'].apply(lambda ts: ts.month)\n",
    "        summer = ((month >= 6) & (month <= 8)).astype('int')\n",
    "        \n",
    "        #winter = ((month == 12) | ((month <= 2) & (month >= 1))).astype('int')\n",
    "        #spring = ((month >= 3) & (month <= 5)).astype('int')\n",
    "        #autumn = ((month >= 9) & (month <= 11)).astype('int')\n",
    "        \n",
    "        # day of the week features\n",
    "        weekday = X['time1'].apply(lambda ts: ts.weekday()).astype('int')\n",
    "        \n",
    "        # year features\n",
    "        year = X['time1'].apply(lambda ts: ts.year).astype('int')\n",
    "        #year_month = X['time1'].apply(lambda t: 100 * t.year + t.month).astype('int')\n",
    "        \n",
    "        #winter.values, spring.values, autumn.values,\n",
    "        X = np.c_[morning.values, day.values, evening.values, summer.values, \\\n",
    "                  weekday.values, year.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add new features, that should be scaled.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # session time features\n",
    "        times = ['time%s' % i for i in range(1, 11)]\n",
    "        # session duration: take to the power of 1/5 to normalize the distribution\n",
    "        session_duration = (X[times].max(axis=1) - X[times].min(axis=1)).astype('timedelta64[ms]').astype(int) ** 0.2\n",
    "        # number of sites visited in a session\n",
    "        number_of_sites = X[times].isnull().sum(axis=1).apply(lambda x: 10 - x)\n",
    "        # average time spent on one site during a session\n",
    "        time_per_site = (session_duration / number_of_sites) ** 0.2\n",
    "        \n",
    "        X = np.c_[session_duration.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pipeline = Pipeline([\n",
    "    (\"preparator\", DataPreparator()),\n",
    "    (\"list_preparator\", ListPreparator()),\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 3), max_features=100000))\n",
    "])\n",
    "\n",
    "attributes_pipeline = Pipeline([\n",
    "    (\"adder\", AttributesAdder())\n",
    "])\n",
    "\n",
    "scaled_attributes_pipeline = Pipeline([\n",
    "    (\"adder\", ScaledAttributesAdder()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "('vectorizer_pipeline', vectorizer_pipeline),\n",
    "('attributes_pipeline', attributes_pipeline),\n",
    "('scaled_attributes_pipeline', scaled_attributes_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = full_pipeline.fit_transform(train_df)\n",
    "X_test = full_pipeline.transform(test_df)\n",
    "\n",
    "y_train = train_df[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9661166697919938"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "#c_values = np.logspace(-2, 2, 20)\n",
    "logit = LogisticRegression(C=0.18, random_state=17, solver='liblinear')\n",
    "#logit = LogisticRegression(random_state=17, solver='lbfgs')\n",
    "\n",
    "#logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_val},\n",
    "#                                  scoring='roc_auc', n_jobs=-1, cv=time_split, verbose=1)\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=1)\n",
    "\n",
    "cv_scores.mean()\n",
    "\n",
    "#logit_grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.18, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=17, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "write_to_submission_file(logit_test_pred, 'custom_pipeline_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!\n",
    "try_train_df = train_df.copy()\n",
    "try_train_df = try_train_df.sort_values(by='time1')\n",
    "\n",
    "try_train_df = train_df[train_df['time1'] >= '2013-05-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ТЕСТ ОБРЕЗКИ ТРЕЙН_ДФ\n",
    "\n",
    "X_train = full_pipeline.fit_transform(try_train_df)\n",
    "X_test = full_pipeline.transform(test_df)\n",
    "\n",
    "y_train = try_train_df[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "c_val = np.logspace(-4, 2, 20)\n",
    "logit_cv = LogisticRegressionCV(Cs=c_val, cv=time_split, scoring='roc_auc', solver='liblinear', n_jobs=-1, random_state=17).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/jupyter_projects/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:2260: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score has changed. The scoring parameter is now used. This warning will disappear in version 0.22.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9896078026252935"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cs': array([1.00000000e-04, 2.06913808e-04, 4.28133240e-04, 8.85866790e-04,\n",
       "        1.83298071e-03, 3.79269019e-03, 7.84759970e-03, 1.62377674e-02,\n",
       "        3.35981829e-02, 6.95192796e-02, 1.43844989e-01, 2.97635144e-01,\n",
       "        6.15848211e-01, 1.27427499e+00, 2.63665090e+00, 5.45559478e+00,\n",
       "        1.12883789e+01, 2.33572147e+01, 4.83293024e+01, 1.00000000e+02]),\n",
       " 'class_weight': None,\n",
       " 'cv': TimeSeriesSplit(max_train_size=None, n_splits=8),\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1.0,\n",
       " 'l1_ratios': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'warn',\n",
       " 'n_jobs': -1,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 17,\n",
       " 'refit': True,\n",
       " 'scoring': 'roc_auc',\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_cv_pp = logit_cv.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51545158e-05, 2.89139224e-04, 2.41186726e-03, ...,\n",
       "       2.21495996e-03, 1.52838074e-06, 2.13227675e-05])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv_pp[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99984845e-01, 1.51545158e-05],\n",
       "       [9.99710861e-01, 2.89139224e-04],\n",
       "       [9.97588133e-01, 2.41186726e-03],\n",
       "       ...,\n",
       "       [9.97785040e-01, 2.21495996e-03],\n",
       "       [9.99998472e-01, 1.52838074e-06],\n",
       "       [9.99978677e-01, 2.13227675e-05]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv_pp#[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit_cv.predict_proba(X_test)[:, 1]\n",
    "\n",
    "write_to_submission_file(logit_test_pred, 'pipeline_cut_try_logitcv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/jupyter_projects/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:2260: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score has changed. The scoring parameter is now used. This warning will disappear in version 0.22.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9895027132458616"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_cv.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9576464237589468"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "logit = LogisticRegression(C=3.56, random_state=17, solver='liblinear')\n",
    "#logit = LogisticRegression(random_state=17, solver='lbfgs')\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "cv_scores.mean()\n",
    "#0.95362 liblinear n_splits=8 C=1\n",
    "#0.93711 lbfgs\n",
    "#0.935049 iblinear n_splits=6\n",
    "#0.93271 iblinear n_splits=5\n",
    "#0.93211 iblinear n_splits=9\n",
    "#0.94867 liblinear n_splits=8 C=0.1\n",
    "#0.95149 liblinear n_splits=8 C=0.2\n",
    "#0.9535 liblinear n_splits=8 C=0.5\n",
    "#0.95398 liblinear n_splits=8 C=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.944043051154898, {'C': 4.3428571428571425})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit_grid_searcher.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "write_to_submission_file(logit_test_pred, 'pipeline_cut_train_gscv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id\n",
       "1        2014-02-20 10:02:45\n",
       "2        2014-02-22 11:19:50\n",
       "3        2013-12-16 16:40:17\n",
       "4        2014-03-28 10:52:12\n",
       "5        2014-02-28 10:53:05\n",
       "                 ...        \n",
       "253557   2013-11-25 10:26:54\n",
       "253558   2013-03-12 16:01:15\n",
       "253559   2013-09-12 14:05:03\n",
       "253560   2013-12-19 15:20:22\n",
       "253561   2014-04-25 09:56:52\n",
       "Name: time1, Length: 253561, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = '2014-02-20 10:02:45'\n",
    "time = pd.to_datetime(time) + relativedelta(months=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-04-30 23:39:53')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_train_df(train_df, test_df, pipeline, date):\n",
    "    print('Train data frame starts at ', date)\n",
    "    \n",
    "    train_df.sort_values(by='time1')\n",
    "    train_df = train_df[train_df['time1'] >= date]\n",
    "    \n",
    "    X_train = full_pipeline.fit_transform(train_df)\n",
    "    X_test = full_pipeline.transform(test_df)\n",
    "\n",
    "    y_train = train_df[\"target\"].astype('int').values\n",
    "    \n",
    "    time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "    logit = LogisticRegression(random_state=17, solver='liblinear')\n",
    "\n",
    "    cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=1)  \n",
    "    print('CV mean score: ', cv_scores.mean())\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "    \n",
    "    logit_test_pred = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    write_to_submission_file(logit_test_pred, 'custom_pipeline_cut_{}.csv'.format(date))\n",
    "    \n",
    "    return cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data frame starts at  2013-01-12 08:05:57\n",
      "CV mean score:  0.9677516470573522\n",
      "Train data frame starts at  2013-02-12 08:05:57\n",
      "CV mean score:  0.9667072829727531\n",
      "Train data frame starts at  2013-03-12 08:05:57\n",
      "CV mean score:  0.9660186820080507\n",
      "Train data frame starts at  2013-04-12 08:05:57\n",
      "CV mean score:  0.9677917323116562\n",
      "Train data frame starts at  2013-05-12 08:05:57\n",
      "CV mean score:  0.9742345155104627\n",
      "Train data frame starts at  2013-06-12 08:05:57\n",
      "CV mean score:  0.9748798329327567\n",
      "Train data frame starts at  2013-07-12 08:05:57\n",
      "CV mean score:  0.9751020897868399\n",
      "Train data frame starts at  2013-08-12 08:05:57\n",
      "CV mean score:  0.9749307387803395\n",
      "Train data frame starts at  2013-09-12 08:05:57\n",
      "CV mean score:  0.9752516749458143\n",
      "Train data frame starts at  2013-10-12 08:05:57\n",
      "CV mean score:  0.9726688916353792\n",
      "Train data frame starts at  2013-11-12 08:05:57\n",
      "CV mean score:  0.9732411211968601\n",
      "Train data frame starts at  2013-12-12 08:05:57\n",
      "CV mean score:  0.9815645171315289\n",
      "Train data frame starts at  2014-01-12 08:05:57\n",
      "CV mean score:  0.9864241075569082\n",
      "Train data frame starts at  2014-02-12 08:05:57\n",
      "CV mean score:  0.9875901210957072\n",
      "Train data frame starts at  2014-03-12 08:05:57\n",
      "CV mean score:  0.9894510338064189\n",
      "CPU times: user 30min 37s, sys: 13min 10s, total: 43min 47s\n",
      "Wall time: 14min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = {}\n",
    "date = train_df['time1'].min()\n",
    "while date < pd.to_datetime('2014-04-01 00:00:00'):\n",
    "    cv_s = cut_train_df(train_df, test_df, full_pipeline, date)\n",
    "    results[date] = cv_s\n",
    "    date = date + relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
