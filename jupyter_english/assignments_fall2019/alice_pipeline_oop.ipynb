{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file, target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = np.arange(1, predicted_labels.shape[0] + 1), columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path('../../data/alice/')\n",
    "\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv(PATH_TO_DATA / 'train_sessions.csv',\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv(PATH_TO_DATA / 'test_sessions.csv',\n",
    "                      index_col='session_id', parse_dates=times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2013-01-12 08:05:57'),\n",
       " Timestamp('2014-04-30 23:39:53'),\n",
       " Timestamp('2014-05-01 17:14:03'),\n",
       " Timestamp('2014-12-05 23:26:53'))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1'].min(), train_df['time1'].max(), test_df['time1'].min(), test_df['time1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 21), (82797, 20))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!\n",
    "try_train_df = try_train_df.sort_values(by='time1')\n",
    "\n",
    "try_train_df = train_df[train_df['time1'] >= '2014-01-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82797 entries, 1 to 82797\n",
      "Data columns (total 20 columns):\n",
      "site1     82797 non-null int64\n",
      "time1     82797 non-null datetime64[ns]\n",
      "site2     81308 non-null float64\n",
      "time2     81308 non-null datetime64[ns]\n",
      "site3     80075 non-null float64\n",
      "time3     80075 non-null datetime64[ns]\n",
      "site4     79182 non-null float64\n",
      "time4     79182 non-null datetime64[ns]\n",
      "site5     78341 non-null float64\n",
      "time5     78341 non-null datetime64[ns]\n",
      "site6     77566 non-null float64\n",
      "time6     77566 non-null datetime64[ns]\n",
      "site7     76840 non-null float64\n",
      "time7     76840 non-null datetime64[ns]\n",
      "site8     76151 non-null float64\n",
      "time8     76151 non-null datetime64[ns]\n",
      "site9     75484 non-null float64\n",
      "time9     75484 non-null datetime64[ns]\n",
      "site10    74806 non-null float64\n",
      "time10    74806 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](10), float64(9), int64(1)\n",
      "memory usage: 13.3 MB\n"
     ]
    }
   ],
   "source": [
    "#train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Fill NaN with zero values.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        sites = ['site%s' % i for i in range(1, 11)]\n",
    "        return X[sites].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListPreparator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Prepare a CountVectorizer friendly 2D-list from data.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.values.tolist()\n",
    "        # Convert dataframe rows to strings\n",
    "        return [\" \".join([str(site) for site in row]) for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add new attributes to training and test set.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    def transform(self, X, y=None):\n",
    "        # intraday features\n",
    "        hour = X['time1'].apply(lambda ts: ts.hour)\n",
    "        morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "        day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "        evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "        \n",
    "        # season features\n",
    "        month = X['time1'].apply(lambda ts: ts.month)\n",
    "        summer = ((month >= 6) & (month <= 8)).astype('int')\n",
    "        \n",
    "        #winter = ((month == 12) | ((month <= 2) & (month >= 1))).astype('int')\n",
    "        #spring = ((month >= 3) & (month <= 5)).astype('int')\n",
    "        #autumn = ((month >= 9) & (month <= 11)).astype('int')\n",
    "        \n",
    "        # day of the week features\n",
    "        weekday = X['time1'].apply(lambda ts: ts.weekday()).astype('int')\n",
    "        \n",
    "        # year features\n",
    "        year = X['time1'].apply(lambda ts: ts.year).astype('int')\n",
    "        #year_month = X['time1'].apply(lambda t: 100 * t.year + t.month).astype('int')\n",
    "        \n",
    "        #winter.values, spring.values, autumn.values,\n",
    "        X = np.c_[morning.values, day.values, evening.values, summer.values, \\\n",
    "                  weekday.values, year.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add new features, that should be scaled.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # session time features\n",
    "        times = ['time%s' % i for i in range(1, 11)]\n",
    "        # session duration: take to the power of 1/5 to normalize the distribution\n",
    "        session_duration = (X[times].max(axis=1) - X[times].min(axis=1)).astype('timedelta64[ms]').astype(int) ** 0.2\n",
    "        # number of sites visited in a session\n",
    "        number_of_sites = X[times].isnull().sum(axis=1).apply(lambda x: 10 - x)\n",
    "        # average time spent on one site during a session\n",
    "        time_per_site = (session_duration / number_of_sites) ** 0.2\n",
    "        \n",
    "        X = np.c_[session_duration.values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pipeline = Pipeline([\n",
    "    (\"preparator\", DataPreparator()),\n",
    "    (\"list_preparator\", ListPreparator()),\n",
    "    (\"vectorizer\", CountVectorizer(ngram_range=(1, 3), max_features=100000))\n",
    "])\n",
    "\n",
    "attributes_pipeline = Pipeline([\n",
    "    (\"adder\", AttributesAdder())\n",
    "])\n",
    "\n",
    "scaled_attributes_pipeline = Pipeline([\n",
    "    (\"adder\", ScaledAttributesAdder()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "('vectorizer_pipeline', vectorizer_pipeline),\n",
    "('attributes_pipeline', attributes_pipeline),\n",
    "('scaled_attributes_pipeline', scaled_attributes_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = full_pipeline.fit_transform(train_df)\n",
    "X_test = full_pipeline.transform(test_df)\n",
    "\n",
    "y_train = train_df[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9661166697919938"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "#c_values = np.logspace(-2, 2, 20)\n",
    "logit = LogisticRegression(C=0.18, random_state=17, solver='liblinear')\n",
    "#logit = LogisticRegression(random_state=17, solver='lbfgs')\n",
    "\n",
    "#logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_val},\n",
    "#                                  scoring='roc_auc', n_jobs=-1, cv=time_split, verbose=1)\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=1)\n",
    "\n",
    "cv_scores.mean()\n",
    "\n",
    "#logit_grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.18, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=17, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "write_to_submission_file(logit_test_pred, 'custom_pipeline_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ТЕСТ ОБРЕЗКИ ТРЕЙН_ДФ\n",
    "\n",
    "X_train = full_pipeline.fit_transform(try_train_df)\n",
    "X_test = full_pipeline.transform(test_df)\n",
    "\n",
    "y_train = try_train_df[\"target\"].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9576464237589468"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "logit = LogisticRegression(C=3.56, random_state=17, solver='liblinear')\n",
    "#logit = LogisticRegression(random_state=17, solver='lbfgs')\n",
    "\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "cv_scores.mean()\n",
    "#0.95362 liblinear n_splits=8 C=1\n",
    "#0.93711 lbfgs\n",
    "#0.935049 iblinear n_splits=6\n",
    "#0.93271 iblinear n_splits=5\n",
    "#0.93211 iblinear n_splits=9\n",
    "#0.94867 liblinear n_splits=8 C=0.1\n",
    "#0.95149 liblinear n_splits=8 C=0.2\n",
    "#0.9535 liblinear n_splits=8 C=0.5\n",
    "#0.95398 liblinear n_splits=8 C=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.944043051154898, {'C': 4.3428571428571425})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit_grid_searcher.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "write_to_submission_file(logit_test_pred, 'pipeline_cut_train_gscv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id\n",
       "1        2014-02-20 10:02:45\n",
       "2        2014-02-22 11:19:50\n",
       "3        2013-12-16 16:40:17\n",
       "4        2014-03-28 10:52:12\n",
       "5        2014-02-28 10:53:05\n",
       "                 ...        \n",
       "253557   2013-11-25 10:26:54\n",
       "253558   2013-03-12 16:01:15\n",
       "253559   2013-09-12 14:05:03\n",
       "253560   2013-12-19 15:20:22\n",
       "253561   2014-04-25 09:56:52\n",
       "Name: time1, Length: 253561, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = '2014-02-20 10:02:45'\n",
    "time = pd.to_datetime(time) + relativedelta(months=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2014-04-30 23:39:53')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data frame starts at  2013-01-12 08:05:57\n",
      "CV mean score:  0.9693551196211163\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-02-12 08:05:57\n",
      "CV mean score:  0.96725091653925\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-03-12 08:05:57\n",
      "CV mean score:  0.966315260041542\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-04-12 08:05:57\n",
      "CV mean score:  0.9673175947500096\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-05-12 08:05:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score:  0.9749204025463137\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-06-12 08:05:57\n",
      "CV mean score:  0.9751043092834284\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-07-12 08:05:57\n",
      "CV mean score:  0.9746160734837436\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-08-12 08:05:57\n",
      "CV mean score:  0.9741023522812794\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-09-12 08:05:57\n",
      "CV mean score:  0.974339675194239\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-10-12 08:05:57\n",
      "CV mean score:  0.9733546036968108\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-11-12 08:05:57\n",
      "CV mean score:  0.9735095674067052\n",
      "\n",
      "\n",
      "Train data frame starts at  2013-12-12 08:05:57\n",
      "CV mean score:  0.9813701260883556\n",
      "\n",
      "\n",
      "Train data frame starts at  2014-01-12 08:05:57\n",
      "CV mean score:  0.9865954352217234\n",
      "\n",
      "\n",
      "Train data frame starts at  2014-02-12 08:05:57\n",
      "CV mean score:  0.9878347277362287\n",
      "\n",
      "\n",
      "Train data frame starts at  2014-03-12 08:05:57\n",
      "CV mean score:  0.9898458185347307\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "date = train_df['time1'].min()\n",
    "while date < pd.to_datetime('2014-04-01 00:00:00'):\n",
    "    cv_s = cut_train_df(train_df, test_df, full_pipeline, date)\n",
    "    results[date] = cv_s\n",
    "    date = date + relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_train_df(train_df, test_df, pipeline, date):\n",
    "    print('Train data frame starts at ', date)\n",
    "    \n",
    "    train_df.sort_values(by='time1')\n",
    "    train_df = train_df[train_df['time1'] >= date]\n",
    "    \n",
    "    X_train = full_pipeline.fit_transform(train_df)\n",
    "    X_test = full_pipeline.transform(test_df)\n",
    "\n",
    "    y_train = train_df[\"target\"].astype('int').values\n",
    "    \n",
    "    time_split = TimeSeriesSplit(n_splits=8)\n",
    "\n",
    "    logit = LogisticRegression(random_state=17, solver='liblinear')\n",
    "\n",
    "    cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                        scoring='roc_auc', n_jobs=1)  \n",
    "    print('CV mean score: ', cv_scores.mean())\n",
    "    print('\\n')\n",
    "    \n",
    "    return cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
